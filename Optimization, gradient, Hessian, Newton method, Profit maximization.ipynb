{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a documention file for what I learnt from a Medium post. \n",
    "Below is the originl link:\n",
    "https://medium.com/towards-data-science/optimization-newtons-method-profit-maximization-part-1-basic-optimization-theory-ff7c5f966565\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Introduction\n",
    "\n",
    "A review of first derivative (Gradient), second derivative (Hessian) and how we utilize the gradient or Newton method to get the optimal value that minimize/maximine the objecive function. \n",
    "\n",
    "Basically, there are two ways to do the optimization:\n",
    "1. grid search iteration. easy to understand but may be conputational expensive \n",
    "2. calculate directly using calculus. calculate the Gradient of the objective function and set it to 0, then solve the optimal value. \n",
    "\n",
    "For complicated objective function, we have to derive both Hessian and Gradient and use Newton's way to get the optimal parameter. \n",
    "\n",
    "Below is the Python code to derive gradient, Hessian given an object function and use Newton's method to get the optimal values. \n",
    "\n",
    "Newton's method:\n",
    "\n",
    "$$x1 = x0 - Hessisn^{-1} * Gradient $$\n",
    "\n",
    "where x0 is the initial guess, it could be a single value or a vector of values, depend on the objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries \n",
    "import sympy as sm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to derive first derivative(gradient) and calculate its value when x=x0 \n",
    "def get_gradient(function, symbols, x0): # Add x0 as argument\n",
    "    '''\n",
    "    Helper function to solve for Gradient of SymPy function.\n",
    "    '''\n",
    "    d1 = {}\n",
    "    gradient = np.array([])\n",
    "\n",
    "    for s in symbols:\n",
    "        d1[s]= sm.diff(function,s).evalf(subs=x0) # add evalf method\n",
    "        gradient = np.append(gradient, d1[s])\n",
    "\n",
    "    return gradient.astype(np.float64) # Change data type to float\n",
    "\n",
    "# a function to derive second derivative(Hessian) and calculate its value when x=x0 \n",
    "def get_hessian(function, symbols, x0): # Add x0 as argument\n",
    "    '''\n",
    "    Helper function to solve for Hessian of SymPy function.\n",
    "    '''\n",
    "    d2 = {}\n",
    "    hessian = np.array([])\n",
    "\n",
    "    for s1 in symbols:\n",
    "        for s2 in symbols:\n",
    "            d2[f\"{s1}{s2}\"] = sm.diff(function,s1,s2).evalf(subs=x0) # add evalf method\n",
    "            hessian = np.append(hessian, d2[f\"{s1}{s2}\"])\n",
    "\n",
    "    hessian = np.array(np.array_split(hessian,len(symbols)))\n",
    "\n",
    "    return hessian.astype(np.float64) # Change data type to float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Newton method to get the optimal value of x and y \n",
    "def newton_method(function,symbols,x0,iterations=100,mute=False):\n",
    "    '''\n",
    "    Function to run Newton's method to optimize SymPy function.\n",
    "    '''\n",
    "    # Dictionary of values to record each iteration\n",
    "    x_star = {}\n",
    "    x_star[0] = np.array(list(x0.values()))\n",
    "\n",
    "    if not mute:\n",
    "        print(f\"Starting Values: {x_star[0]}\")\n",
    "\n",
    "    i=0\n",
    "    while i < iterations:\n",
    "        \n",
    "        # Get gradient and hessian\n",
    "        gradient = get_gradient(function, symbols, dict(zip(x0.keys(),x_star[i])))\n",
    "        hessian = get_hessian(function, symbols, dict(zip(x0.keys(),x_star[i])))\n",
    "        \n",
    "        # Newton method iteration scheme\n",
    "        x_star[i+1] = x_star[i].T - np.linalg.inv(hessian) @ gradient.T\n",
    "        \n",
    "        # Check convergence criteria\n",
    "        if np.linalg.norm(x_star[i+1] - x_star[i]) < 10e-5:\n",
    "            solution = dict(zip(x0.keys(),x_star[i+1]))\n",
    "            print(f\"\\nConvergence Achieved ({i+1} iterations): Solution = {solution}\")\n",
    "            break \n",
    "        else: \n",
    "            solution = None\n",
    "        \n",
    "        if not mute:\n",
    "            print(f\"Step {i+1}: {x_star[i+1]}\")\n",
    "        \n",
    "        i += 1\n",
    "   \n",
    "    return solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define symbols & objective function\n",
    "x, y = sm.symbols('x y')\n",
    "\n",
    "# initialize the starting value of x and y, and it is totally randomo \n",
    "Gamma = [x,y]\n",
    "\n",
    "# define the objective function\n",
    "objective = 100*(y-x**2)**2 + (1-x)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-215.6,  -88. ])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the gradient when x= -1.2, y = 1.0 \n",
    "get_gradient(objective, Gamma,{x:-1.2,y:1.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1330.,  480.],\n",
       "       [ 480.,  200.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the Hessian when x= -1.2, y = 1.0 \n",
    "get_hessian(objective, Gamma,{x:-1.2,y:1.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Values: [-1.2  1. ]\n",
      "Step 1: [-1.1752809   1.38067416]\n",
      "Step 2: [ 0.76311487 -3.17503385]\n",
      "Step 3: [0.76342968 0.58282478]\n",
      "Step 4: [0.99999531 0.94402732]\n",
      "Step 5: [0.9999957  0.99999139]\n",
      "\n",
      "Convergence Achieved (6 iterations): Solution = {x: 0.9999999999999999, y: 0.9999999999814724}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{x: 0.9999999999999999, y: 0.9999999999814724}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newton_method(objective,Gamma,{x:-1.2,y:1.0},iterations=100,mute=False)\n",
    "\n",
    "# after 6 interations, we get the optimal value of x and y that can minimize the objecctive function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Contrained Optimization\n",
    "\n",
    "Constrained optimization is defined as “the process of optimizing an objective function with respect to some variables in the presence of constraints on those variables.\n",
    "\n",
    "There are two types of contraints (equality and inequality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Equality Constraint - The Lagrangian\n",
    "\n",
    "$$ \\mathcal{L} = f(x) + \\sum_{j=1}^{r}\\lambda_j h_j(x) $$\n",
    "\n",
    "$\\mathcal{L}$ represents the Lagrangian objective function, f(x) is the original objective function, $\\sum_{j=1}^{r}\\lambda_j h_j(x)$ is the sum of Lagrange multipliers to each constraint $j$\n",
    "\n",
    "The goal is the find the optimal value of Xs,Lagrange multipliers that maximize or minimaze the Lagrangian objective function using Newton's method "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:\n",
    "$$ min\\;100(y-x^2)^2+(1-x)^2, $$\n",
    "$$ s.t. x^2-y=2$$\n",
    "\n",
    "then the Lagrangian function will be:\n",
    "$$ \\mathcal{L} = 100(y-x^2)^2+(1-x)^2+\\lambda*(x^2-y-2)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Values: [-1.2  1.   1. ]\n",
      "Step 1: [  -1.17555556   -0.61866667 -400.        ]\n",
      "Step 2: [   0.7677616   -5.1870237 -400.       ]\n",
      "Step 3: [   0.76806868   -1.4100706  -400.        ]\n",
      "Step 4: [   0.99999563   -1.05379886 -400.        ]\n",
      "Step 5: [   0.999996   -1.000008 -400.      ]\n",
      "\n",
      "Convergence Achieved (6 iterations): Solution = {x: 0.9999999999999999, y: -1.0000000000160152, λ: -400.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{x: 0.9999999999999999, y: -1.0000000000160152, λ: -400.0}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y, λ  = sm.symbols('x y λ')\n",
    "\n",
    "Langrangian_objective = 100*(y-x**2)**2 + (1-x)**2 + λ*(x**2-y-2)\n",
    "Gamma = [x,y,λ]\n",
    "Gamma0 = {x:-1.2,y:1,λ:1}\n",
    "\n",
    "newton_method(Langrangian_objective,Gamma,Gamma0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Inequality Constraints - The Logarithmic Barrier Function \n",
    "\n",
    "$$\\min_x\\;f(x), x = [x_1,x_2,...,x_n] \\in \\mathbb{R}$$\n",
    "$$ g_j(x) \\le 0, j = 1,2,...,m  $$\n",
    "f(x) is the objective functiono and g(x) is the contraint, we have m constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interior-point methods/barrier methods \n",
    "$$ \\mathcal{B}(x,\\rho) = f(x) - \\rho\\sum_{j=1}^m\\;log(c_j(x))$$\n",
    "$$x = [x_1,x_2,...,x_n], c_j(x) = \\left\\{\\begin{array}{rcl} g_j(x)&g_j(x)\\ge0 \\\\ -g_j(x)&g_j(x)\\le0\\end{array}\\right.$$\n",
    "where $\\rho$ is a small positive scalar, also known as the barrier parameter, $c(x)$ states that depending on constraints ($\\ge 0 \\;or \\le 0 $) will dictate whether we use the negative or positive of that constraint. c(x) should be formulated to be always greater than 0.\n",
    "\n",
    "How does it works?\n",
    "\n",
    "As the optimal values approach the “barrier” outlined by the constraint, this method relies on the fact that the logarithmic function approaches negative infinity as the value approaches zero, thereby penalizing the objective function value. As ρ → 0, the penalization decreases and we converge to the solution. However, it is necessary to start with a sufficiently large ρ so that the penalization is large enough to prevent “jumping” out of the barriers. Therefore, the algorithm has one extra loop than Newton’s method alone — namely, we choose a starting value ρ, optimize the barrier function using Newton’s method, then update ρ by slowly decreasing it (ρ → 0), and repeat until convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:\n",
    "$$ min\\;100(y-x^2)^2+(1-x)^2, $$\n",
    "$$ s.t. x \\le 0, y \\ge 3$$\n",
    "\n",
    "then the Barrier Lagrangian function will be:\n",
    "$$ \\mathcal{B} = 100(y-x^2)^2+(1-x)^2-\\rho*log(((y-3)(-x)))$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the function for inequality constraints  \n",
    "def constrained_newton_method(function,symbols,x0,iterations=100,mute=False):\n",
    "    '''\n",
    "    Function to run Barrier method & Newton's method to optimize a\n",
    "    constrained SymPy function.\n",
    "    '''  \n",
    "    \n",
    "    # Values to record over each iteration\n",
    "    x_star = {}\n",
    "    x_star[0] = np.array(list(x0.values())[:-1])\n",
    "    optimal_solutions = []\n",
    "    optimal_solutions.append(dict(zip(list(x0.keys())[:-1],x_star[0])))\n",
    "    \n",
    "    # Barrier Method Iteration \n",
    "    step = 1 \n",
    "    while step < iterations:\n",
    "\n",
    "        # Starting rho\n",
    "        if step == 1: \n",
    "            rho_sub = list(x0.values())[-1]\n",
    "\n",
    "        # Evaluate function at rho value\n",
    "        rho_sub_values = {list(x0.keys())[-1]:rho_sub}\n",
    "        function_eval = function.evalf(subs=rho_sub_values)\n",
    "\n",
    "        if not mute:\n",
    "            print(f\"Step {step} w/ {rho_sub_values}\")\n",
    "            print(f\"Starting Values: {x_star[0]}\")\n",
    "        \n",
    "        # Newton's Method\n",
    "        i=0\n",
    "        while i < iterations:\n",
    "        \n",
    "            gradient = get_gradient(function_eval, symbols[:-1], dict(zip(list(x0.keys())[:-1],x_star[i])))\n",
    "            hessian = get_hessian(function_eval, symbols[:-1], dict(zip(list(x0.keys())[:-1],x_star[i])))\n",
    "\n",
    "            x_star[i+1] = x_star[i].T - np.linalg.inv(hessian) @ gradient.T\n",
    "\n",
    "            if np.linalg.norm(x_star[i+1] - x_star[i]) < 10e-5:\n",
    "                solution = dict(zip(list(x0.keys())[:-1],x_star[i+1]))\n",
    "                if not mute:\n",
    "                    print(f\"Convergence Achieved ({i+1} iterations): Solution = {solution}\\n\") \n",
    "                break\n",
    "            \n",
    "            i += 1\n",
    "\n",
    "        # Record optimal solution for each barrier method iteration\n",
    "        optimal_solution = x_star[i+1]\n",
    "        previous_optimal_solution = list(optimal_solutions[step-1].values())\n",
    "        optimal_solutions.append(dict(zip(list(x0.keys())[:-1],optimal_solution)))\n",
    "        \n",
    "        # Check for overall convergence\n",
    "        if np.linalg.norm(optimal_solution - previous_optimal_solution) < 10e-5:\n",
    "            print(f\"\\n Overall Convergence Achieved ({step} steps): Solution = {optimal_solutions[step]}\\n\")\n",
    "            break\n",
    "\n",
    "        # Set new starting point\n",
    "        x_star = {}\n",
    "        x_star[0] = optimal_solution\n",
    "\n",
    "        # Update rho\n",
    "        rho_sub = 0.9*rho_sub\n",
    "\n",
    "        # Update Steps\n",
    "        step += 1\n",
    "\n",
    "    return optimal_solutions[step]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, ρ = sm.symbols('x y ρ')\n",
    "\n",
    "Barrier_objective = 100*(y-x**2)**2 + (1-x)**2 - ρ*sm.log((-x)*(y-3))\n",
    "Gamma = [x,y,ρ] # Function requires last symbol to be ρ!\n",
    "Gamma0 = {x:-15,y:15,ρ:10}  # set up the initial values \n",
    "\n",
    "#constrained_newton_method(Barrier_objective,Gamma,Gamma0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
